{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on how linear least squares problems for model fitting (design matrix, kernel, lines, polynomials, affine, and other multivariate functions) and the interpretation of results for various types of models (see week 7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square problems\n",
    "\n",
    "Design matrix: With a design matrix multiplied with a weights-vector for a given model allows us to compare the model's predicted values to observed values to calculate the residual sum of squares and evaluate our model.  \n",
    "// Kernel: Creating a kernel function honestly not sure  \n",
    "Lines: is essentially just a 1st order polynomial, see below  \n",
    "Polynomials: Learning a model based on data points, where we evaluate a model based on how well it describes existing data  \n",
    "Affine transformations: We need to map a picture of atrium to x,y coordinates for further processing  \n",
    "Multivariate functions: We want to learn how many people we expect to be on the bus based on various independent inputs (weather, weekday, ...)  \n",
    "\n",
    "## Goal\n",
    "\n",
    "We aim to minimize the \"error\" of a model, essentially comparing how close each observed data point is to a model's predicted data.  \n",
    "The general formula is $||Ax-b||^2$ having $A$ as the design matrix, $x$ containing model parameters (\"weights\") and b representing observed data points.  \n",
    "\n",
    "We can then project the observed points $b$ onto the column space of $A$ allowing us to minimize for the residuals (differences between observed $b$ and the projections $\\hat{b}$ on $A$).\n",
    "\n",
    "- Lines: $A = \\begin{bmatrix}1 & x\\end{bmatrix}$ \n",
    "- Polynomials: $A = \\begin{bmatrix}1 & ... & x^n \\end{bmatrix}$ (for a polynomial of $n$'th degree)\n",
    "- Affine transformations: $A$ represents one or more translations and transformations\n",
    "- Multivariate functions: $A = \\begin{bmatrix}1 & x_1 & ... & x_n \\end{bmatrix}$ (for $n$ inputs)\n",
    "\n",
    "## Solution\n",
    "\n",
    "By projecting, we can find a solution such that $x$ is the model parameters giving the least squared errors as follows:  \n",
    "$x = (A^TA)^{-1}A^Tb$\n",
    "\n",
    "$x$ can be interpreted as the model parameters where for a line it would be the slope and intercept, and a polynomial it would contain coefficients to $x^0 ... x^n$. For multivariate models, it would each be a weight of each independent variable.\n",
    "\n",
    "The residuals are then vector subtraction between $b$ (observed) and $Ax$ (predicted) data points. These residuals are noise or unmodeled variation.   "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
