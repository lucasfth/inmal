{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on inner products, vector operations, distance metrics and their relation to ML (evaluation and other metrics). You may relate this to week 10 (evaluation) but focus on vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "1. Introduction to vectors\n",
    "2. Vector operations\n",
    "3. Distance metrics\n",
    "4. Relation to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectors\n",
    "\n",
    "Vectors are used to represent data points in `n` dimensions for vectors of size `n`. Vectors allows us to do different operations on them.\n",
    "\n",
    "### Inner product\n",
    "\n",
    "Two vector's inner product for two vectors $u$ and $v$ are written as $u \\cdot v$ and is calculated as $u \\cdot v = \\sum_{i=1}^n u_i * v_i$.\n",
    "\n",
    "The inner product describes the angle and magnitude between the two vectors, such that an inner product of 0 means that the two vectors are orthogonal meaning they create a right angle. For a positive value, the vectors form an acute angle and for a negative value, the vectors form an obtuse angle.\n",
    "\n",
    "It also describes the magnitude between the two vectors, meaning that we in machine learning can use it to describe how similar two data points are. Inner product creates a scalar value whereas the other vector operations produce a new vector.\n",
    "\n",
    "It is worth noting that conceptually we can calculate the inner product of two same-dimensionality vectors, e.g. two vectors in $\\mathbb{R}^N$. Since we later learn of Matrix Multiplication and we might use matrices programatically to represent the vectors $A$ and $B$ that are both in $\\mathbb{R}^{N \\times 1}$, we know that $A \\times B$ is not possible, but we can transpose one and calculate the same yielding the same result for the matrices with exactly one row or column.\n",
    "\n",
    "#### Orthogonality\n",
    "\n",
    "If the inner product of two vectors equal zero, they form a right angle ($90^\\circ$). Take the two vectors $[3, 4] \\cdot [8, -6]$. The inner product is $(3 * 8) + (4 * (-6)) = 0$. If we drew them in a graph we would see a right angle formed between the two vectors. We cannot determine parallelism just as simple, but we can divide the inner product with the product of the lengths of the vectors, essentially normalizing them (see below). If the result is 0 (equivalent to $\\frac{0}{||u|| * ||v||}$) the vectors are orthogonal, whereas a value of 1 indicates two vectors in the exact same direction and a value of -1 for vectors in direct opposite directions (both of these are parallel). This calculation is essentially the measurement of cosine similarity.\n",
    "\n",
    "### Addition\n",
    "\n",
    "Combines two vectors component-wise: $u + v = [u_1+v_1, ..., u_n+v_n]$.\n",
    "\n",
    "The visualization of adding two vectors component-wise can be described as extending vector $u$ with vector $v$. It is also possible to subtract two vectors component-wise similarly.\n",
    "\n",
    "Can be used to compare vectors such as computing residuals, meaning the differences between predictions and targets. *Possibly also something about composite features*.\n",
    "\n",
    "### Scalar multiplication\n",
    "\n",
    "Instead of multiplying two vectors component-wise as we did with the inner product, scalar multiplication multiplies all components of a vector $u$ with some scalar $c$ as follows: $c \\cdot u = [c \\cdot u_1, ..., c \\cdot u_n]$. We can use it to scale features or modify vector directions.\n",
    "\n",
    "### Normalization\n",
    "\n",
    "Normalization converts a vector to unit length (magnitude of 1) which is used to optimize models. Can also be used to ensure no dominating features when grouping data points using e.g. k-means clustering or when comparing vectors with cosine similarity.\n",
    "\n",
    "It is done by calculating the magnitude of the vector and dividing each component of the vector with this value and is written formally as $\\hat{v} = \\frac{v}{||v||}$. The magnitude $||v||$ of the vector is calculated using the Euclidean norm $||v||=\\sqrt{\\sum_{i=1}^n v_i^2}$.\n",
    "\n",
    "### Distance metrics\n",
    "\n",
    "Above we discussed both Euclidean distance and Cosine Similarity, which are two different ways to calculate similarity between two vectors.\n",
    "Euclidean distance calculates the distance in a straight line between two points by subtracting the two vectors from each other, squaring each component, then sum and lastly take the squareroot of this value. Cosine similarity uses angles to determine similarity  $\\cos(\\theta) = \\frac{u \\cdot v}{||u|| ||v||}$. A third method for distances is the Manhattan distance which is somewhat similar to the Euclidean distance, but for each component we just sum the absolute value of the two vector's component-wise subtraction.\n",
    "\n",
    "Distance metrics are used for nearest neighbor and clustering of algorithms, e.g. when evaluating models.\n",
    "\n",
    "With a calculation of the cosine similarity, we can also extract the following data:\n",
    "- A value of 0 = orthogonal vectors, meaning they form a right angle ($90^\\circ$)\n",
    "- A value of 1 = Parallel vectors (both pointing in the exact same direction)\n",
    "- A value of -1 = Parallel vectors (pointing in exact opposite directions)\n",
    "- A value of $\\cos{\\theta} > 0$ indicates the vectors are both pointing in the same general direction (less than a 90 degree angle between them)\n",
    "- A value of $\\cos{\\theta} < 0$ indicates the vectors are pointing in opposite general directions (larger than a 90 degree angle between them)\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "We use these distance metrics to train a model by comparing the expected result with actual result. Consider some model parameters and some input variables and we know the output variables, we can then use the model parameters and input variables to find some expected output variables, and the difference between this result and the actual output variables can then be compared across multiple model parameters to determine the most fitting solution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
